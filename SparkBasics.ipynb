{"cells":[{"cell_type":"markdown","source":["1. **Immutable DF**\n2. **Lazy eval**\n3. **Col vs SQL expr**\n4. **Filter pushdown**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"18c443f4-256c-4629-b218-9b11afc8a564","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Session\ndf1 = spark.read \\\n            .option('path', \"/FileStore/NYSEData\") \\\n            .format('csv') \\\n            .option(\"inferSchema\", \"true\") \\\n            .load()\n\n# Jobs > Stages > Tasks\n# 1 Statement can triger 1 job or more \n# 1 job can have 1 or many Stages\n# 1 stage can have 1 or more tasks"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"99688752-8f3c-4886-b34a-c2e7a47d1b46","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df1 = df1.withColumnRenamed(\"_c0\", \"ShortName\").withColumnRenamed(\"_c1\", \"Date\").withColumnRenamed(\"_c2\", \"StockValue\").select(\"ShortName\", \"Date\", \"StockValue\")\ndisplay(df1)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"01edc3f3-d4fc-418e-9486-e65b19786a71","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Column expression vs SQL expression\nfrom pyspark.sql.functions import col\n\ndf2_sql = df1.where(\"Date = 20160101\").where(\"`ShortName` like 'A%'\")  # SQL Expr\n\ndf2_col = df1.where(col('Date') == 20160101).where(col('ShortName').like('A%'))  # Col EXPR\n\n# transformation / action\n# T1(where) > T2(filter) > T3(groupby) > Action(display of DF after T3) > T4 > T5 > Action(collect)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1cfe34f3-0d49-40a5-bf45-7b03751fd4b1","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Lazy Eval - Actions and Transformations\n\ndisplay(df2_sql)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c4fabdd1-26d1-4802-afa1-d969c43e7818","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Immutable DF\n\nprint(df2_sql.rdd.id(), df1.rdd.id())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1ae46546-1887-4b27-be83-6fa5319b877e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Filter push down analysis\n\ndf2_sql = df2_sql.where(\"`Date` = '20160101'\")\ndf2_sql.explain()\n\n# Get all data backend > filter in spark > NO PUSHDOWN\n# Filter data in backend > Show in spark > Pushdown"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8ac71a80-c7f5-4341-938a-6c7d4f5e3444","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Datatype and Schema definition\nfrom pyspark.sql.types import StructField, StructType, StringType, DateType\n\ntrain_df_schema = StructType([\n    StructField('ShortName', StringType()), \n    StructField('Date', DateType()), \n    StructField('StockValue', StringType())])\ndf3_explicit_schema = spark.read \\\n                             .option('path', \"/FileStore/NYSEData\") \\\n                             .format('csv') \\\n                             .schema(train_df_schema) \\\n                             .option('dateFormat', \"yyyymmdd\") \\\n                             .load()\n\nprint(\"Previous data types are \", df2_sql.schema, '\\n')\nprint(\"Explicitly defined data types are \", df3_explicit_schema.schema)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7c5dacdf-e1c3-4ecb-a2c4-aef71e719b13","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(df3_explicit_schema)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0b90ee8b-508b-4343-ae1b-677587bef27d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Filter push down analysis\n\nprint(df3_explicit_schema.where(\"StockValue > 50\").explain())\n\ndf4_explicit_schema = df3_explicit_schema\n# Filter was not done in back end. ALL data was retrieved by SPARK and then filteration was done in SPARK"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"56906c5a-9426-4a59-827d-bc8285b69c1f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["1. **Simple aggregation**\n2. **Grouped aggregation**\n3. **Window aggregation**\n4. **Cache**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7046f268-6ed0-42aa-ae1c-bdab06f8fb4d","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Aggregations\nfrom pyspark.sql.functions import avg, count, expr\n\n# Simple Aggregations - SQL expressions\ndisplay(df4_explicit_schema.selectExpr(\"count(1) as `Total Rows`\", \"avg(StockValue) as `Average StockValue`\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"775a2603-6fa0-4e65-b60b-b076957056ee","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Group Aggregations - SQL Expressions\n\ndf4_explicit_schema.createOrReplaceTempView(\"df4_explicit_schema_tempview\")\ndf5_explicit_schema = spark.sql(\"\"\"select \n                                  `Date`, \n                                  avg(StockValue) as `Aggregated StockValue` \n                                 from \n                                  df4_explicit_schema_tempview \n                                 where `ShortName` like ('A%')\n                                 group by \n                                  `Date`\n                                 having sum(StockValue) > 100\n                                 order by \n                                  `Date` \n                                \"\"\")\n\ndisplay(df5_explicit_schema)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b4c9975a-d79a-4ea5-9e95-336ebca141f9","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Cache\n\n# df4_explicit_schema.cache()\n# df4_explicit_schema.take(1)\n\ndf4_explicit_schema.is_cached\n\n# df4_explicit_schema.storageLevel\n# df4_explicit_schema.unpersist()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ce8e4aaa-6567-4abf-b973-f69e1e2d4d41","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Window Func Running Sum\n\nfrom pyspark.sql.functions import sum\nfrom pyspark.sql import Window\n\n# Preperatory Steps\ndf4_explicit_schema = df4_explicit_schema.withColumnRenamed(\"StockValue\", \"Sales\")\ndf4_explicit_schema = df4_explicit_schema.groupBy(\"ShortName\", \"Date\").agg(sum(\"Sales\").alias(\"Sales\")).orderBy(\"ShortName\", \"Date\")\n\nspark_sales_custom_window1 = Window.partitionBy(\"ShortName\").orderBy(\"Date\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\nsales_df3 = df4_explicit_schema.withColumn(\"RunningTotalSales\", sum(\"Sales\").over(spark_sales_custom_window1))\n\ndisplay(sales_df3)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f40f6576-02f0-46c6-9bf1-021b05854018","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Window Func mean\n\nfrom pyspark.sql.functions import mean\n\nspark_sales_custom_window2 = Window.partitionBy(\"ShortName\").orderBy(\"Date\").rowsBetween(-2, Window.currentRow)\nsales_df4 = df4_explicit_schema.withColumn(\"MovingAverageSales\", mean(\"Sales\").over(spark_sales_custom_window2))\n\ndisplay(sales_df4)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d8ee97ea-6ff2-44f2-8f18-f77274e9e167","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Window Func Rank\n\nfrom pyspark.sql.functions import dense_rank\n\nsales_df5 = df4_explicit_schema.select(\"Date\", \"ShortName\", \"Sales\")  # Preperatory Step\n\nspark_sales_custom_window3 = Window.partitionBy(\"Date\").orderBy(\"Sales\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\nsales_df5 = sales_df5.withColumn(\"RankSales\", dense_rank().over(spark_sales_custom_window3))\ndisplay(sales_df5)\n\n# df1 .........df4_explicit_schema > sales_df5\n# df1..........df4_explicit_schema > sales_df4\n\n# df1 .........df4_explicit_schema (cache) > sales_df5\n#              df4_explicit_schema (cache)> sales_df4\n\n\n\n# Dense Rank - 5 Jobs with 1 Stage each\n\n# DF1(CSV) > DF2(T) > DF3 > DF4 > DF5 > DF6(action - display)\n# DF1(CSV) > DF2(T) > DF3 > DF7 > DF8(action - display)\n\n# DF1(CSV) > DF2(T) > DF3(CACHE) > DF4 > DF5 > DF6(action - display)\n#                     DF3(CACHE) > DF7 > DF8(action - display)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"890f6ad0-3ec6-4067-97ae-4b039c629f89","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["1. **Repartition**\n2. **Max records per file**\n3. **Bucketing**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"04aa2045-2dd0-4628-a573-b6a8aac61c6a","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Write data, file formats and files created upon write\n\ndf4_explicit_schema.write \\\n                     .format('parquet') \\\n                     .mode('overwrite') \\\n                     .option('path', \"/FileStore/ParquetOutput0/\") \\\n                     .save()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0d55c0d8-8989-4a5d-beb4-c6e0546bfeb1","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Write data, file formats and files created upon write\nfrom pyspark.sql.functions import spark_partition_id\n\nprint(\"number of partitions of DF are\", df4_explicit_schema.rdd.getNumPartitions())\nprint(\"\\npartitions size of DF are shown below\")\nprint(df4_explicit_schema.groupBy(spark_partition_id()).count().show())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9fb0a3af-25ad-4f5b-b237-acd4cc553225","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Write data, file formats and files created upon write\n\ndf4_explicit_schema = df4_explicit_schema.repartition(6)\nprint(\"number of partitions of DF are\", df4_explicit_schema.rdd.getNumPartitions())\n\nprint(\"\\npartitions size of DF are shown below\")\nprint(df4_explicit_schema.groupBy(spark_partition_id()).count().show())\n\ndf4_explicit_schema.write \\\n                     .format('json') \\\n                     .mode('overwrite') \\\n                     .option('path', \"/FileStore/ParquetOutputRepartitioned0/\") \\\n                     .save()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"72ac7168-aaef-4324-ba4c-0d03fed146ff","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Write data, file formats and files created upon write\n\ndf4_explicit_schema.write \\\n                     .format('json') \\\n                     .mode('overwrite') \\\n                     .option('path', \"/FileStore/JSONOutput0/\") \\\n                     .option('maxRecordsPerFile', 100000) \\\n                     .save()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3a7e9352-c5b4-4aed-8cd7-30487937a28a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Write data, file formats and files created upon write\n\njson_output_df = spark.read \\\n                      .format('json') \\\n                      .option('path', '/FileStore/JSONOutput0/part-00000-tid-*-c000.json') \\\n                      .load()\ndisplay(json_output_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"18d12d7d-8310-4f54-b35e-78ffbc4326b9","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Write data, file formats and files created upon write\nfrom pyspark.sql.functions import lower, col\n\nprint(\"\\npartitions size of DF are shown below\")\nprint(df4_explicit_schema.groupBy(spark_partition_id()).count().show())\n\ndf4_explicit_schema.write \\\n                     .format('json') \\\n                     .mode('overwrite') \\\n                     .option('path', \"/FileStore/JSONOutputPartitioned0/\") \\\n                     .partitionBy('Date') \\\n                     .save()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fde861d1-5a68-4b5f-b0be-a48dcf207923","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Write data, file formats and files created upon write\n\njson_output_df2 = spark.read \\\n                      .format('json') \\\n                      .option('path', '/FileStore/JSONOutputPartitioned0/Date=1997-01-01/part-00000-tid-*.c000.json') \\\n                      .load()\ndisplay(json_output_df2)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e410ec40-3538-43e6-b471-850ab61cac99","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Bucketing \n\ndf4_explicit_schema.write \\\n                     .format('json') \\\n                     .mode('overwrite') \\\n                     .bucketBy(2, 'ShortName') \\\n                     .saveAsTable(\"bucket_table_0\")\n\n# PartitionBy - Date (31 Values - Jan) - Partitionby - Date - 1 Jan - 10,000 stocks - 1 file - 1JanFolder - 31 Folders\n# 3100 - 3100 folders .....\n\n# Bucket - HASH key - 2"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"92e63963-1946-4cbd-bbc1-c3a69f8ad97a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["1. **UDF**\n2. **Scalar UDF**\n3. **Grouped UDF**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"551a2cf0-8df5-4c76-b4af-96cd156cb00e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["display(df4_explicit_schema)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a7a62709-cf75-40ac-a02e-90cf47bb0fd8","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# UDF\nfrom pyspark.sql.functions import upper\n\n# UDF - Convert all product values in spark_sales_df6 column into upper case using withColumn and Select\n@udf('string')\ndef udf_func(ShortName):\n    return ShortName.lower()\n\nsales_df5 = sales_df5.select(\"Date\", \"ShortName\", \"Sales\")\nsales_df7 = sales_df5.withColumn(\"UpdatedShortName_UDF\", udf_func(\"ShortName\"))\nsales_df7 = sales_df7.withColumn(\"UpdatedProductName_Spark\", lower(col(\"ShortName\")))\ndisplay(sales_df7)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fb2d8c7a-7db4-406a-a407-06000aba6cec","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Pandas Scalar Vectorized (Arrow) UDF \n\nfrom pyspark.sql.functions import pandas_udf, PandasUDFType\n\n# UDF - Convert all product values in spark_sales_df8 column into upper case using withColumn\n@pandas_udf('string', PandasUDFType.SCALAR)\ndef func_scalarUDF(ShortName):\n    return ShortName.upper()\n\nsales_df8 = sales_df7.withColumn(\"UpdatedProductName_Pandas_Scalar_UDF\", func_scalarUDF(\"ShortName\"))\ndisplay(sales_df8)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"45f1a0ca-d5c1-4453-acd7-9f46d4d2968f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Pandas Scalar Vectorized (Arrow) UDF \n\n@pandas_udf('string', PandasUDFType.SCALAR)\ndef func_scalarUDF(ShortName):\n    return ShortName.str.upper()\n\nsales_df8 = sales_df7.withColumn(\"UpdatedProductName_Pandas_Scalar_UDF\", func_scalarUDF(\"ShortName\"))\ndisplay(sales_df8)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9172fb66-afd5-4651-85c9-947867120200","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Pandas Grouped Map Vectorized (Arrow) UDF \n\nfrom pyspark.sql.types import IntegerType, DoubleType\n\nschema = StructType([\n    StructField('ShortName', StringType(), True),\n    StructField('Date', DateType(), True),\n    StructField('Sales', DoubleType(), True),\n    StructField('running_total_sales_pandasUDF', DoubleType(), True)\n])\n@pandas_udf(schema, functionType=PandasUDFType.GROUPED_MAP)\ndef grouped_map_UDF(group_of_shortnames):\n    print(group_of_shortnames.head())\n    group_of_shortnames[\"running_total_sales_pandasUDF\"] = group_of_shortnames[\"Sales\"].cumsum()\n    return group_of_shortnames\n\nsales_df9 = sales_df8.select(\"ShortName\", \"Date\", \"Sales\")\nsales_df10 = sales_df9.groupBy('ShortName').apply(grouped_map_UDF)\n\ndisplay(sales_df10)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3dc06fae-8d90-415c-b9db-febacff7da98","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Pandas Grouped Map Vectorized (Arrow) UDF \n\n# Test UDF as a python function on drive.\ntestDF = sales_df9.where(\"ShortName = 'A'\").toPandas()\ngrouped_map_UDF.func(testDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"25b321f7-2e14-4a7e-9182-f5ba3bd423e8","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Pandas Grouped Map Vectorized (Arrow) UDF \n\n# Try generating same output via spark native\nsales_df11 = sales_df10.withColumn(\"RunningTotalSales\", sum(\"Sales\").over(spark_sales_custom_window1))\n\n# Try generating same output via spark native function\ndef spark_native_func1(df):\n    df = df.withColumn(\"RunningTotalSales_SeperateFunction\", sum(\"Sales\").over(spark_sales_custom_window1))\n    return df\n\nsales_df12 = spark_native_func1(sales_df11)\ndisplay(sales_df12)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b1abea60-2be3-4ac7-9eb3-8f12568fd25a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["1. **Broadcast Joins**\n2. **Shuffle Sort Merge Joins**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a757ad18-25e2-47f3-aa90-2418e4b36015","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["display(df1)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b651b14b-6bd0-42cf-92dc-886f46c801a6","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["meta_df = spark.read \\\n                 .option('path', \"/FileStore/NYSEMetadata\") \\\n                 .format('csv') \\\n                 .option('delimiter', \"|\") \\\n                 .load()\nmeta_df = meta_df.withColumnRenamed(\"_c0\", \"ShortName\").withColumnRenamed(\"_c1\", \"FullName\")\n\nprint(meta_df.groupBy(spark_partition_id()).count().show())\n\ndf1.createOrReplaceTempView(\"df_view\")\nmeta_df.createOrReplaceTempView(\"meta_df_view\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f0eb6ff4-036f-40c0-a1ba-24789b6e8641","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql \n-- Broadcast Joins\n\nEXPLAIN\nselect \n  table_a.ShortName,   table_b.FullName as FN\nfrom \n  df_view as table_a \n  join meta_df_view as table_b \n  on table_a.ShortName = table_b.ShortName\nwhere \n  table_b.FullName = 'Alcoa Inc.'\n\n/*\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Project [ShortName#119, FullName#108 AS FN#203]\n   +- BroadcastHashJoin [ShortName#119], [ShortName#96], Inner, BuildRight, false\n      :- Project [_c0#45 AS ShortName#119]\n      :  +- Filter isnotnull(_c0#45)\n      :     +- FileScan csv [_c0#45] Batched: false, DataFilters: [isnotnull(_c0#45)], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/FileStore/nysedata], PartitionFilters: [], PushedFilters: [IsNotNull(_c0)], ReadSchema: struct<_c0:string>\n      +- BroadcastExchange HashedRelationBroadcastMode(ArrayBuffer(input[0, string, true]),false), [plan_id=284]\n         +- Project [_c0#76 AS ShortName#96, _c1#77 AS FullName#108]\n            +- Filter ((isnotnull(_c1#77) AND (_c1#77 = Alcoa Inc.)) AND isnotnull(_c0#76))\n               +- FileScan csv [_c0#76,_c1#77] Batched: false, DataFilters: [isnotnull(_c1#77), (_c1#77 = Alcoa Inc.), isnotnull(_c0#76)], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/FileStore/nysemetadataMAIN], PartitionFilters: [], PushedFilters: [IsNotNull(_c1), EqualTo(_c1,Alcoa Inc.), IsNotNull(_c0)], ReadSchema: struct<_c0:string,_c1:string>\n*/\n\n-- Inferences:\n-- explain plan - table_b (short name and full name columns) is broadcased \n-- explain plan - FullName filter on table_b is pushed to the source"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"implicitDf":true},"nuid":"2a20e032-fd45-4b19-a71e-fb8392f9e509","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql \n-- Broadcast Joins\n\nselect \n  table_a.ShortName,   table_b.FullName as FN\nfrom \n  df_view as table_a \n  join meta_df_view as table_b \n  on table_a.ShortName = table_b.ShortName\nwhere \n  table_b.FullName = 'Alcoa Inc.'\n  \n-- Observe the broadcast join happening in logs - SQL/Dataframe"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"implicitDf":true},"nuid":"dc9e6928-39a2-479f-89d6-1f68eb79de29","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Join column Ambiguity in spark\n\noutput_df = meta_df.join(df1, meta_df.ShortName == df1.ShortName, \"inner\").select(\"ShortName\", \"FullName\").where(\"FullName = 'Alcoa Inc.'\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f57d6d71-a911-4e7c-a149-133051e4dce0","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Join column Ambiguity in spark\n\noutput_df = meta_df.join(df1, meta_df.ShortName == df1.ShortName, \"inner\").drop(df1.ShortName).select(\"ShortName\", \"FullName\").where(\"FullName = 'Alcoa Inc.'\")\n\noutput_df.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8964196f-58e2-423a-a3e6-488da38109b7","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# change broadcast join minimum MB limit of small DF to force a shuffle sort join and not broadcast join\n\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 1)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"646439c3-90b5-4b51-88e1-9ead239d5e73","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql \n--Shuffle Sort Merge Join\n\nEXPLAIN\nselect \n  table_a.ShortName,   table_b.FullName as FN\nfrom \n  df_view as table_a \n  join meta_df_view as table_b \n  on table_a.ShortName = table_b.ShortName\nwhere \n  table_b.FullName = 'Alcoa Inc.'\n  \n/*\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Project [ShortName#921, FullName#910 AS FN#968]\n   +- SortMergeJoin [ShortName#921], [ShortName#898], Inner\n      :- Sort [ShortName#921 ASC NULLS FIRST], false, 0\n      :  +- Exchange hashpartitioning(ShortName#921, 200), ENSURE_REQUIREMENTS, [plan_id=1966]\n      :     +- Project [_c0#847 AS ShortName#921]\n      :        +- Filter isnotnull(_c0#847)\n      :           +- FileScan csv [_c0#847] Batched: false, DataFilters: [isnotnull(_c0#847)], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/FileStore/nysedata], PartitionFilters: [], PushedFilters: [IsNotNull(_c0)], ReadSchema: struct<_c0:string>\n      +- Sort [ShortName#898 ASC NULLS FIRST], false, 0\n         +- Exchange hashpartitioning(ShortName#898, 200), ENSURE_REQUIREMENTS, [plan_id=1967]\n            +- Project [_c0#878 AS ShortName#898, _c1#879 AS FullName#910]\n               +- Filter ((isnotnull(_c1#879) AND (_c1#879 = Alcoa Inc.)) AND isnotnull(_c0#878))\n                  +- FileScan csv [_c0#878,_c1#879] Batched: false, DataFilters: [isnotnull(_c1#879), (_c1#879 = Alcoa Inc.), isnotnull(_c0#878)], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/FileStore/nysemetadataMAIN], PartitionFilters: [], PushedFilters: [IsNotNull(_c1), EqualTo(_c1,Alcoa Inc.), IsNotNull(_c0)], ReadSchema: struct<_c0:string,_c1:string>\n*/\n\n-- Inferences:\n-- explain plan - Observe shuffle sort in plan"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"implicitDf":true},"nuid":"2245900b-33ec-4ea2-83b9-5d650178e58d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql \n--Shuffle Sort Merge Join\n\nselect \n  table_a.ShortName,   table_b.FullName as FN\nfrom \n  df_view as table_a \n  join meta_df_view as table_b \n  on table_a.ShortName = table_b.ShortName\nwhere \n  table_b.FullName = 'Alcoa Inc.'\n\n-- Inferences:\n-- Logs - Observe Shuffle sort happening in logs - SQL/DataFrame"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"implicitDf":true},"nuid":"4c22916f-de98-45aa-8bcc-0fd8856abdda","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["1. **AQE - Shuffle Partitions**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b81c9358-adcc-442c-b960-1c3d7f0afa73","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Default shuffle partitions is 200 and it kicks in once you do a wide transformation to an existing DF. \n\nspark.conf.set(\"spark.sql.adaptive.enabled\", False)\nprint(spark.conf.get(\"spark.sql.adaptive.enabled\"), spark.conf.get(\"spark.sql.shuffle.partitions\"))\nprint(df1.rdd.getNumPartitions())\n\ndist_shortname = df1.selectExpr(\"ShortName\").distinct()\nprint(dist_shortname.rdd.getNumPartitions())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cace3833-6fba-488e-9584-da3021b860d2","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Above behaviour can be changed by hardcoding the no of shuffle partitions\n\nspark.conf.set(\"spark.sql.shuffle.partitions\", 10)\ndist_shortname = df1.selectExpr(\"ShortName\").distinct()\nprint(dist_shortname.rdd.getNumPartitions())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a6a3770e-063f-4e1f-849b-90782e540f95","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Above behaviour can also be changed by switching on AQE which will dynamically make partitions as per available cores.\n\nspark.conf.set(\"spark.sql.adaptive.enabled\", True)\ndist_shortname = df1.selectExpr(\"ShortName\").distinct()\nprint(dist_shortname.rdd.getNumPartitions())\ndisplay(dist_shortname)\n\n# Observe logs (SQL/Dataframe) for additional AQE Shuffle which reduces the partitions from 200 to 1"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4a866230-678d-4ee8-ba5d-58a20bc3a716","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"SparkBasics1","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":1040168122555251,"dataframes":["_sqldf"]}},"language":"python","widgets":{},"notebookOrigID":3756661272240590}},"nbformat":4,"nbformat_minor":0}
